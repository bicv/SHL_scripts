{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Hebbian Learning: basics\n",
    "\n",
    "We are interested here in learning the \"optimal\" components of a set of images (let's say some \"natural\", usual images). As there is no supervisor to guide the learning, this is called unsupervised learning. Our basic hypothesis to find the best (\"optimal\") components will be to assume that *a priori* the most sparse is more plausible. We will implement the derived algorithm in this set of scripts.\n",
    "\n",
    "Here, we will show the basic operations that are implemented in this package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T11:22:45.865772Z",
     "start_time": "2018-05-11T11:22:45.848916Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T11:22:46.185993Z",
     "start_time": "2018-05-11T11:22:45.868049Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "\n",
    "To test and control for the role of different parameters, we will have a first object (in the [shl_experiments.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_experiments.py) script) that controls a learning experiment. It contains all relevant parameters, but can also keep a trace of the history of some statistics. This is useful to compare the relative efficiency of the different solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T11:22:46.206974Z",
     "start_time": "2018-05-11T11:22:46.188795Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-bfe6a34b3339>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-bfe6a34b3339>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    indx = np.random.permutation(data.shape[0])[:record_num_batches]====\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tag = 'coding'\n",
    "homeo_methods = ['None', 'HAP', 'HEH']\n",
    "\n",
    "\n",
    "record_num_batches = 2**12\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "from shl_scripts.shl_experiments import SHL\n",
    "shl = SHL()\n",
    "data = shl.get_data(matname=tag)\n",
    "indx = np.random.permutation(data.shape[0])[:record_num_batches]====\n",
    "\n",
    "list_figures = []\n",
    "\n",
    "dico = {}\n",
    "for homeo_method in homeo_methods:\n",
    "    print(19*'üê∂' + homeo_method + 10*'üê∂')\n",
    "    shl = SHL(homeo_method=homeo_method)\n",
    "    dico[homeo_method] = shl.learn_dico(data=data, list_figures=list_figures, matname=tag + '_' + homeo_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T11:22:46.207988Z",
     "start_time": "2018-05-11T11:22:45.840Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coding_gain = {}\n",
    "for homeo_method in homeo_methods:\n",
    "    print(42*'üê∂')\n",
    "    print(15*'üê∂' + homeo_method[:3] + 15*'üê∂')\n",
    "    print(42*'üê∂')\n",
    "    shl = SHL(homeo_method=homeo_method)\n",
    "    dico_ = dico[homeo_method]\n",
    "    print(42*'üêí')\n",
    "    from shl_scripts.shl_encode import mp\n",
    "\n",
    "    sparse_code = mp(data[indx, :], dico_.dictionary, l0_sparseness=shl.l0_sparseness, \n",
    "                     #                           P_cum=dico_.P_cum, gain=None\n",
    "                                              P_cum=None, gain=np.ones(shl.n_dictionary),\n",
    "\n",
    "                        )\n",
    "\n",
    "    #from shl_scripts.shl_learn import get_P_cum\n",
    "    #P_cum = get_P_cum(sparse_code, C=shl.C, nb_quant=shl.nb_quant)\n",
    "    #P_cum.shape\n",
    "\n",
    "    P_cum = dico_.P_cum\n",
    "    #P_cum.shape\n",
    "\n",
    "    from shl_scripts.shl_tools import plot_P_cum\n",
    "    fig, ax = plot_P_cum(P_cum, verbose=False);\n",
    "    #P_cum.mean(axis=0).shape\n",
    "\n",
    "    P_cum_mean = P_cum.mean(axis=0)[np.newaxis, :] * np.ones((shl.n_dictionary, shl.nb_quant))\n",
    "    fig, ax = plot_P_cum(P_cum_mean, fig=fig, ax=ax, c='b', verbose=False)\n",
    "    plt.show()\n",
    "\n",
    "    from shl_scripts.shl_encode import quantile, rescaling\n",
    "    #sparse_code = mp(data, dico.dictionary, l0_sparseness=l0_sparseness, P_cum=P_cum)\n",
    "    stick = np.arange(shl.n_dictionary)*shl.nb_quant\n",
    "    q = quantile(P_cum, rescaling(sparse_code, C=shl.C), stick, do_fast=True)\n",
    "\n",
    "    from shl_scripts.shl_encode import inv_quantile, inv_rescaling\n",
    "    q_sparse_code = inv_rescaling(inv_quantile(P_cum, q), C=shl.C)\n",
    "    print('total deviation of coefficients = ', np.mean(np.abs(q_sparse_code-sparse_code)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(sparse_code.ravel(), q_sparse_code.ravel(), c='b', alpha=.2);\n",
    "\n",
    "\n",
    "    #q_sparse_code = inv_rescaling(inv_quantile(P_cum_mean, q), C=shl.C)\n",
    "    q_sparse_code = inv_rescaling(inv_quantile(P_cum_mean, q), C=shl.C)\n",
    "    print('total deviation of coefficients = ', np.mean(np.abs(sparse_code)))\n",
    "    print('total deviation of coefficients = ', np.mean(np.abs(q_sparse_code-sparse_code)))\n",
    "\n",
    "    #fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(sparse_code.ravel(), q_sparse_code.ravel(), c='r', alpha=.2);\n",
    "    plt.show()\n",
    "\n",
    "    #patches = q_sparse_code @ dico_.dictionary\n",
    "    #error = data[indx, :] - patches\n",
    "    from shl_scripts.shl_tools import print_stats\n",
    "    SD, SE = print_stats(data[indx, :], dico[homeo_method].dictionary, q_sparse_code, verbose=False, display=False)\n",
    "\n",
    "#     from shl_scripts.shl_tools import print_stats\n",
    "#     fig, axs = show_data(error[:max_patches, :], cmax=np.max(np.abs(data[:max_patches, :])))\n",
    "#     print('average of data patches = ', data.mean(), '+/-', data.mean(axis=1).std())\n",
    "#     print('average of residual patches = ', error.mean(), '+/-', error.mean(axis=1).std())\n",
    "#     #SD = np.sqrt(np.mean(data[indx, :]**2, axis=1))\n",
    "#     SD = np.linalg.norm(data[indx, :])/record_num_batches\n",
    "#     print('median energy of data = ', np.median(SD))\n",
    "#     print('average energy of data = ', SD.mean(), '+/-', SD.std())\n",
    "#     #print('total energy of data = ', np.sqrt(np.sum(data**2)))\n",
    "#     #print('total deviation of data = ', np.sum(np.abs(data)))\n",
    "#     #SE = np.sqrt(np.mean(error**2, axis=1))\n",
    "#     SE = np.linalg.norm(error)/record_num_batches\n",
    "\n",
    "#     print('average energy of residual = ', SE.mean(), '+/-', SE.std())\n",
    "#     print('median energy of residual = ', np.median(SE))\n",
    "#     #print('total energy of residual = ', np.sqrt(np.sum(error**2)))\n",
    "#     #print('total deviation of residual = ', np.sum(np.abs(error)))\n",
    "#     print('average gain of coding = ', (SD/SE).mean(), '+/-', (SD/SE).std())\n",
    "#     plt.show()\n",
    "    \n",
    "    coding_gain[homeo_method] = (SD/SE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T11:22:46.209072Z",
     "start_time": "2018-05-11T11:22:45.842Z"
    }
   },
   "outputs": [],
   "source": [
    "coding_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T11:22:46.210406Z",
     "start_time": "2018-05-11T11:22:45.844Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, shl_scripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
