{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Hebbian Learning: toward a quantitative measure of the quality of filters\n",
    "\n",
    "We are interested here in learning the \"optimal\" components of a set of images (let's say some \"natural\", usual images). As there is no supervisor to guide the learning, this is called unsupervised learning. Our basic hypothesis to find the best (\"optimal\") components will be to assume that *a priori* the most sparse is more plausible. We will implement the derived algorithm in this set of scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:32:32.718498Z",
     "start_time": "2018-03-09T20:32:32.696277Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:32:33.400698Z",
     "start_time": "2018-03-09T20:32:32.721217Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "\n",
    "To test and control for the role of different parameters, we will have a first object (in the [shl_experiments.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_experiments.py) script) that controls a learning experiment. It contains all relevant parameters, but can also keep a trace of the history of some statistics. This is useful to compare the relative efficiency of the different solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:32:34.505243Z",
     "start_time": "2018-03-09T20:32:33.405163Z"
    }
   },
   "outputs": [],
   "source": [
    "tag = 'independance'\n",
    "homeo_methods = ['None', 'HAP']\n",
    "homeo_methods = ['None', 'HEH']\n",
    "\n",
    "record_num_batches = 2**12\n",
    "N_show = 120\n",
    "ymin = .95\n",
    "max_patches = 10\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "from shl_scripts.shl_experiments import SHL\n",
    "shl = SHL()\n",
    "data = shl.get_data(matname=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:32:34.822752Z",
     "start_time": "2018-03-09T20:32:34.507768Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff    2358345 Mar  9 16:33 data_cache/independance_HAP_dico.pkl\r\n",
      "-rw-r--r--  1 laurentperrinet  staff    2564918 Mar  8 18:41 data_cache/independance_None_dico.pkl\r\n",
      "-rw-r--r--  1 laurentperrinet  staff  924830048 Mar  8 17:14 data_cache/independance_data.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {shl.data_cache}/{tag}*\n",
    "!rm -fr {shl.data_cache}/{tag}*lock*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T15:47:51.122211Z",
     "start_time": "2018-03-08T15:47:50.519250Z"
    }
   },
   "source": [
    "!rm {shl.data_cache}/{tag}*\n",
    "!ls -l {shl.data_cache}/{tag}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:32:36.315654Z",
     "start_time": "2018-03-09T20:32:34.827180Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patches, size of patches =  (262140, 441)\n",
      "average of patches =  -9.76666621861323e-06  +/-  0.007136243820971976\n",
      "average energy of data =  0.08754999528870613 +/- 0.0544347348997371\n"
     ]
    }
   ],
   "source": [
    "print('number of patches, size of patches = ', data.shape)\n",
    "print('average of patches = ', data.mean(), ' +/- ', data.mean(axis=1).std())\n",
    "SE = np.sqrt(np.mean(data**2, axis=1))\n",
    "print('average energy of data = ', SE.mean(), '+/-', SE.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning\n",
    "\n",
    "The actual learning is done in a second object (here ``dico``) from which we can access another set of properties and functions  (see the [shl_learn.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_learn.py) script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.691Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dictionary = (number of filters, size of imagelets) =  (529, 441)\n",
      "average of filters =  6.689843987932055e-05 +/- 0.0029564290169642455\n",
      "average energy of filters =  1.0 +/- 8.234341763631113e-17\n"
     ]
    }
   ],
   "source": [
    "list_figures = ['show_dico', 'time_plot_error', 'time_plot_logL', 'show_Pcum']\n",
    "\n",
    "dico = {}\n",
    "for homeo_method in homeo_methods:\n",
    "    shl = SHL(homeo_method=homeo_method)\n",
    "    dico[homeo_method] = shl.learn_dico(data=data, list_figures=list_figures, matname=tag + '_' + homeo_method)\n",
    "\n",
    "    print('size of dictionary = (number of filters, size of imagelets) = ', dico[homeo_method].dictionary.shape)\n",
    "    print('average of filters = ',  dico[homeo_method].dictionary.mean(axis=1).mean(), \n",
    "          '+/-',  dico[homeo_method].dictionary.mean(axis=1).std())\n",
    "    SE = np.sqrt(np.sum(dico[homeo_method].dictionary**2, axis=1))\n",
    "    print('average energy of filters = ', SE.mean(), '+/-', SE.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## theory\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T15:47:38.856955Z",
     "start_time": "2018-02-15T15:47:38.796854Z"
    }
   },
   "source": [
    "sparse_code_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.711Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "# define the mean\n",
    "rho = shl.l0_sparseness / shl.n_dictionary\n",
    "\n",
    "# draw binomial samples\n",
    "sparse_code_binary = np.random.rand(record_num_batches, shl.n_dictionary) > 1 - rho\n",
    "\n",
    "# define the theoretical standard deviation\n",
    "sd = np.sqrt(rho*(1-rho)*record_num_batches)\n",
    "\n",
    "# the binomial probability is defined from 0 to record_num_batches \\times shl.n_dictionary\n",
    "# for visualization, we show the part around non-zero probabilities:\n",
    "#record_num_batches_high = int(rho * record_num_batches + 15. * sd)\n",
    "record_num_batches_high = int(2.5 * rho * record_num_batches)\n",
    "bins = np.arange(record_num_batches_high)\n",
    "\n",
    "print('Sum of non-zero coefficients', sparse_code_binary.sum(), '~=', shl.l0_sparseness*record_num_batches)\n",
    "print('average non-zeros', np.count_nonzero(sparse_code_binary, axis=0).mean(), '~=', rho*record_num_batches)\n",
    "fig, ax= plt.subplots(figsize=(13, 5))\n",
    "smarts, edges = np.histogram(np.count_nonzero(sparse_code_binary, axis=0), density=True, bins=bins)\n",
    "smarts /= smarts.sum()\n",
    "ax.step(edges, np.hstack((smarts, 0)), where='pre', label='data');\n",
    "#edges_mid = .5*( edges[1:] + edges[:-1] )\n",
    "#print(rho, s, edges)\n",
    "proba = np.exp(-.5 * (edges - rho*record_num_batches)**2 / sd**2)\n",
    "proba /= proba.sum()\n",
    "ax.plot(edges, proba, label='gaussian pmf');\n",
    "proba_binom = binom.pmf(edges, record_num_batches, rho)\n",
    "proba_binom /= proba_binom.sum()\n",
    "ax.plot(edges, proba_binom, 'bo', ms=1, label='binom pmf')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.716Z"
    }
   },
   "outputs": [],
   "source": [
    "from shl_scripts.shl_tools import get_logL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T13:42:16.983239Z",
     "start_time": "2018-03-09T13:42:16.891208Z"
    }
   },
   "source": [
    "def get_logL(sparse_code):\n",
    "    record_num_batches = sparse_code.shape[0]\n",
    "    rho_hat = np.count_nonzero(sparse_code_binary, axis=0).mean()/record_num_batches\n",
    "    #rho = shl.l0_sparseness / shl.n_dictionary\n",
    "    sd = np.sqrt(rho_hat*(1-rho_hat)*record_num_batches)\n",
    "\n",
    "    measures = np.count_nonzero(sparse_code, axis=0)\n",
    "\n",
    "    # likelihood = 1 / np.sqrt(2*np.pi) / sd *  np.exp(-.5 * (measures - rho)**2 / sd**2)\n",
    "    logL = -.5 * (measures - rho_hat*record_num_batches)**2 / sd**2\n",
    "    logL -= np.log(np.sqrt(2*np.pi) * sd)\n",
    "    #print(np.log(np.sum(np.exp(logL))), np.log(np.sqrt(2*np.pi) * sd))\n",
    "    #logL -= np.log(np.sum(np.exp(logL)))\n",
    "    return logL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.739Z"
    }
   },
   "outputs": [],
   "source": [
    "rho = shl.l0_sparseness / shl.n_dictionary\n",
    "sd = np.sqrt(rho*(1-rho)*record_num_batches)\n",
    "\n",
    "for _ in range(10):\n",
    "    sparse_code_binary = np.random.rand(record_num_batches, shl.n_dictionary) > 1 - rho\n",
    "    logL = get_logL(sparse_code_binary)\n",
    "    print ('log-likelihood for random sample=', logL.mean(), '+/-', logL.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coding\n",
    "\n",
    "The learning itself is done via a gradient descent but is highly dependent on the coding / decoding algorithm. This belongs to a another function (in the [shl_encode.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_encode.py) script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.741Z"
    }
   },
   "outputs": [],
   "source": [
    "from shl_scripts.shl_tools import show_data\n",
    "def print_stats(data, dictionary, sparse_code):\n",
    "    print(42*'🐒')\n",
    "\n",
    "    print('number of codes, size of codewords = ', sparse_code.shape)\n",
    "    print('average of codewords = ', sparse_code.mean())\n",
    "    print('average std of codewords = ', sparse_code.std())\n",
    "    print('l0-sparseness of codewords = ', (sparse_code>0).mean(), ' ~= l0/M =', shl.l0_sparseness/shl.n_dictionary)\n",
    "    print('std of the average of individual patches = ', sparse_code.mean(axis=0).std())\n",
    "\n",
    "\n",
    "    plt.matshow(sparse_code[:N_show, :])\n",
    "    plt.show()\n",
    "    fig, axs = show_data(data[:max_patches, :])\n",
    "    plt.show()\n",
    "\n",
    "    patches = sparse_code @ dictionary\n",
    "    error = data - patches\n",
    "    \n",
    "    print('number of codes, size of reconstructed images = ', patches.shape)\n",
    "\n",
    "    fig, axs = show_data(patches[:max_patches, :])\n",
    "    plt.show()\n",
    "    fig, axs = show_data(error[:max_patches, :], cmax=np.max(np.abs(patches[:max_patches, :])))\n",
    "    plt.show()\n",
    "    print('average of data patches = ', data.mean(), '+/-', data.mean(axis=1).std())\n",
    "    print('average of residual patches = ', error.mean(), '+/-', error.mean(axis=1).std())\n",
    "    SD = np.sqrt(np.mean(data**2, axis=1))\n",
    "\n",
    "    print('median energy of data = ', np.median(SD))\n",
    "    print('average energy of data = ', SD.mean(), '+/-', SD.std())\n",
    "    \n",
    "    SE = np.sqrt(np.mean(error**2, axis=1))\n",
    "\n",
    "    print('average energy of residual = ', SE.mean(), '+/-', SE.std())\n",
    "    print('median energy of residual = ', np.median(SE))\n",
    "    print('average gain of coding = ', (SD/SE).mean(), '+/-', (SD/SE).std())\n",
    "    \n",
    "    return SD, SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.747Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shl_scripts.shl_encode import sparse_encode\n",
    "sparse_code = {}\n",
    "for homeo_method in homeo_methods:\n",
    "    print(42*'🐶')\n",
    "    print(19*'🐶' + homeo_method + 10*'🐶')\n",
    "    print(42*'🐶')\n",
    "    \n",
    "    if dico[homeo_method].P_cum is None: \n",
    "        gain = np.ones(shl.n_dictionary)\n",
    "    else:\n",
    "        gain = None\n",
    "    sparse_code[homeo_method] = sparse_encode(data[indx, :], dico[homeo_method].dictionary, \n",
    "                                 P_cum=dico[homeo_method].P_cum, l0_sparseness=shl.l0_sparseness, C=shl.C)\n",
    "    SD, SE = print_stats(data[indx, :], dico[homeo_method].dictionary, sparse_code[homeo_method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.750Z"
    }
   },
   "outputs": [],
   "source": [
    "sparse_code[homeo_method].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.753Z"
    }
   },
   "outputs": [],
   "source": [
    "rho = shl.l0_sparseness / shl.n_dictionary\n",
    "sd = np.sqrt(rho*(1-rho)*record_num_batches)\n",
    "\n",
    "fig, ax= plt.subplots(figsize=(13, 5))\n",
    "for homeo_method, color in zip(homeo_methods, ['r', 'b']):\n",
    "    print(42*'🐶')\n",
    "    print(19*'🐶' + homeo_method + 10*'🐶')\n",
    "    print(42*'🐶')\n",
    "    print('Average of non-zero coefficients', (sparse_code[homeo_method]>0).mean(), '~=', rho)\n",
    "    n_i = np.count_nonzero(sparse_code[homeo_method], axis=0)\n",
    "    print('Mean of non-zero coefficients', n_i.mean(), '~=', rho*record_num_batches)\n",
    "    print('Mean of non-zero coefficients', n_i.mean(), '+/-', n_i.std())\n",
    "    print('Median of non-zero coefficients', np.median(n_i))\n",
    "    smarts, edges = np.histogram(n_i, density=True, bins=bins)\n",
    "    smarts /= smarts.sum()\n",
    "    # ax.step(edges, np.hstack((smarts, 0)), where='pre', label='data_' + homeo_method);\n",
    "    ax.fill_between(edges[:-1], smarts, step='pre', label='data_' + homeo_method, alpha=.4, color=color);\n",
    "\n",
    "proba = 1 / np.sqrt(2*np.pi) / sd *  np.exp(-.5 * (edges - rho*record_num_batches)**2 / sd**2)\n",
    "#proba /= proba.sum()\n",
    "ax.plot(edges, proba, label='expected', c='g', lw=4)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.755Z"
    }
   },
   "outputs": [],
   "source": [
    "rho, sparse_code[homeo_method].mean(), homeo_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measuring the distance to independance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.758Z"
    }
   },
   "outputs": [],
   "source": [
    "#fig, ax= plt.subplots(figsize=(13, 5))\n",
    "for homeo_method in homeo_methods:\n",
    "    logL = get_logL(sparse_code[homeo_method])\n",
    "    print ('log-likelihood for homeo_method=', homeo_method, 'is', logL.mean(), '+/-', logL.std())\n",
    "    #ax.vline(edges, proba, label=homeo_method)\n",
    "\n",
    "logL = get_logL(np.random.rand(record_num_batches, shl.n_dictionary) > 1 - rho)\n",
    "print ('log-likelihood for theory is', logL.mean(), '+/-', logL.std())\n",
    "#for _ in range(10):\n",
    "#    logL = get_logL(np.random.rand(record_num_batches, shl.n_dictionary) > 1 - rho)\n",
    "#ax.plot(edges, proba, label='theory')\n",
    "#ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-09T20:32:32.762Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, shl_scripts, pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
