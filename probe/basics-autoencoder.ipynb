{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Hebbian Learning: toward a quantitative measure of the quality of filters\n",
    "\n",
    "We are interested here in learning the \"optimal\" components of a set of images (let's say some \"natural\", usual images). As there is no supervisor to guide the learning, this is called unsupervised learning. Our basic hypothesis to find the best (\"optimal\") components will be to assume that *a priori* the most sparse is more plausible. We will implement the derived algorithm in this set of scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:13.685181Z",
     "start_time": "2018-03-06T16:02:13.667149Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:14.297208Z",
     "start_time": "2018-03-06T16:02:13.687481Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "\n",
    "To test and control for the role of different parameters, we will have a first object (in the [shl_experiments.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_experiments.py) script) that controls a learning experiment. It contains all relevant parameters, but can also keep a trace of the history of some statistics. This is useful to compare the relative efficiency of the different solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:14.338830Z",
     "start_time": "2018-03-06T16:02:14.300405Z"
    }
   },
   "outputs": [],
   "source": [
    "DEBUG_DOWNSCALE, verbose = 10, 0\n",
    "DEBUG_DOWNSCALE, verbose = 1, 0\n",
    "DEBUG_DOWNSCALE, verbose = 10, 10\n",
    "DEBUG_DOWNSCALE, verbose = 1, 10\n",
    "\n",
    "homeo_method = 'HEH'\n",
    "tag = 'autoencoder'\n",
    "# tag = 'independance'\n",
    "\n",
    "matname = tag +'_' + homeo_method\n",
    "\n",
    "nb_quant = 128\n",
    "nb_quant = 256\n",
    "C = 5.\n",
    "eta_homeo = 0.05\n",
    "alpha_homeo = 0.02\n",
    "do_double_shuffle = True\n",
    "do_double_shuffle = False\n",
    "do_random = True\n",
    "\n",
    "n_iter = 2**13 + 1\n",
    "\n",
    "record_num_batches = 2**11\n",
    "N_show = 120\n",
    "ymin = .95\n",
    "max_patches = 10\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "opts = dict(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, n_iter=n_iter, \n",
    "            homeo_method=homeo_method, eta_homeo=eta_homeo, alpha_homeo=alpha_homeo, \n",
    "            C=C, nb_quant=nb_quant, P_cum=None, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:14.607415Z",
     "start_time": "2018-03-06T16:02:14.341664Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data..loading the data called : data_cache/autoencoder_data\n",
      "Data is of shape : (262140, 144) - done in 0.24s.\n"
     ]
    }
   ],
   "source": [
    "from shl_scripts.shl_experiments import SHL\n",
    "shl = SHL(**opts)\n",
    "data = shl.get_data(matname=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:14.884571Z",
     "start_time": "2018-03-06T16:02:14.610663Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff    2533678 Mar  6 13:14 data_cache/autoencoder_HEH_dico.pkl\r\n",
      "-rw-r--r--  1 laurentperrinet  staff    2533679 Mar  6 13:42 data_cache/autoencoder_None_dico.pkl\r\n",
      "-rw-r--r--  1 laurentperrinet  staff  301985408 Mar  6 10:39 data_cache/autoencoder_data.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {shl.data_cache}/{tag}*\n",
    "!rm -fr {shl.data_cache}/{tag}*lock*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:15.208299Z",
     "start_time": "2018-03-06T16:02:14.888296Z"
    }
   },
   "source": [
    "!rm {shl.data_cache}/{tag}*\n",
    "!ls -l {shl.data_cache}/{tag}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:15.572403Z",
     "start_time": "2018-03-06T16:02:15.211819Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patches, size of patches =  (262140, 144)\n",
      "average of patches =  -2.2939303312336248e-05  +/-  0.008160732348805759\n",
      "average energy of data =  0.059551743099862775 +/- 0.07023889938824754\n"
     ]
    }
   ],
   "source": [
    "print('number of patches, size of patches = ', data.shape)\n",
    "print('average of patches = ', data.mean(), ' +/- ', data.mean(axis=1).std())\n",
    "SE = np.sqrt(np.mean(data**2, axis=1))\n",
    "print('average energy of data = ', SE.mean(), '+/-', SE.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:15.595289Z",
     "start_time": "2018-03-06T16:02:15.574868Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0_sparseness = shl.l0_sparseness\n",
    "l0_sparseness_noise = 200 #shl.n_dictionary #\n",
    "#l0_sparseness_high = shl.l0_sparseness * 2\n",
    "shl.do_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T16:02:15.625834Z",
     "start_time": "2018-03-06T16:02:15.597496Z"
    }
   },
   "outputs": [],
   "source": [
    "stick = np.arange(shl.n_dictionary)*nb_quant\n",
    "\n",
    "indx = np.random.permutation(data.shape[0])[:record_num_batches]\n",
    "P_cum_zeroeffect = np.linspace(0, 1, nb_quant, endpoint=True)[np.newaxis, :] * np.ones((shl.n_dictionary, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning\n",
    "\n",
    "The actual learning is done in a second object (here ``dico``) from which we can access another set of properties and functions  (see the [shl_learn.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_learn.py) script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.678Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache found data_cache/autoencoder_HEH_dico.pkl: Learning the dictionary with algo = mp \n",
      " Training on 262140 patches\n",
      "Iteration   1 /   8193 (elapsed time:   1s,   0mn   1s)\n",
      "Iteration  129 /   8193 (elapsed time:  189s,   3mn   9s)\n",
      "Iteration  257 /   8193 (elapsed time:  641s,  10mn  41s)\n",
      "Iteration  385 /   8193 (elapsed time:  1088s,  18mn   8s)\n",
      "Iteration  513 /   8193 (elapsed time:  1531s,  25mn  31s)\n",
      "Iteration  641 /   8193 (elapsed time:  1978s,  32mn  58s)\n",
      "Iteration  769 /   8193 (elapsed time:  2424s,  40mn  24s)\n",
      "Iteration  897 /   8193 (elapsed time:  4658s,  77mn  38s)\n",
      "Iteration  1025 /   8193 (elapsed time:  8604s,  143mn  24s)\n",
      "Iteration  1153 /   8193 (elapsed time:  12233s,  203mn  53s)\n",
      "Iteration  1281 /   8193 (elapsed time:  15353s,  255mn  53s)\n",
      "Iteration  1409 /   8193 (elapsed time:  22085s,  368mn   5s)\n",
      "Iteration  1537 /   8193 (elapsed time:  27942s,  465mn  42s)\n",
      "Iteration  1665 /   8193 (elapsed time:  34012s,  566mn  52s)\n",
      "Iteration  1793 /   8193 (elapsed time:  38818s,  646mn  58s)\n",
      "Iteration  1921 /   8193 (elapsed time:  44829s,  747mn   9s)\n",
      "Iteration  2049 /   8193 (elapsed time:  50083s,  834mn  43s)\n"
     ]
    }
   ],
   "source": [
    "list_figures = ['show_dico', 'time_plot_error', 'time_plot_logL']\n",
    "\n",
    "dico_homeo = shl.learn_dico(data=data, list_figures=list_figures, matname=matname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.681Z"
    }
   },
   "outputs": [],
   "source": [
    "print('size of dictionary = (number of filters, size of imagelets) = ', dico_homeo.dictionary.shape)\n",
    "print('average of filters = ',  dico_homeo.dictionary.mean(axis=1).mean(), \n",
    "      '+/-',  dico_homeo.dictionary.mean(axis=1).std())\n",
    "SE = np.sqrt(np.sum(dico_homeo.dictionary**2, axis=1))\n",
    "print('average energy of filters = ', SE.mean(), '+/-', SE.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.683Z"
    }
   },
   "outputs": [],
   "source": [
    "if not dico_homeo.P_cum is None: \n",
    "    from shl_scripts.shl_tools import plot_P_cum\n",
    "    fig, ax = plot_P_cum(dico_homeo.P_cum, ymin=ymin, verbose=False, alpha=.15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coding\n",
    "\n",
    "The learning itself is done via a gradient descent but is highly dependent on the coding / decoding algorithm. This belongs to a another function (in the [shl_encode.py](https://github.com/bicv/SHL_scripts/blob/master/shl_scripts/shl_encode.py) script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.685Z"
    }
   },
   "outputs": [],
   "source": [
    "from shl_scripts.shl_tools import show_data\n",
    "def print_stats(data, dictionary, sparse_code):\n",
    "    print(42*'🐒')\n",
    "    patches = sparse_code @ dictionary\n",
    "    error = data - patches\n",
    "\n",
    "    print('number of codes, size of codewords = ', sparse_code.shape)\n",
    "    print('average of codewords = ', sparse_code.mean())\n",
    "    print('average std of codewords = ', sparse_code.std())\n",
    "    print('l0-sparseness of codewords = ', (sparse_code>0).mean(), ' ~= l0/M =', shl.l0_sparseness/shl.n_dictionary)\n",
    "    print('std of the average of individual patches = ', sparse_code.mean(axis=0).std())\n",
    "\n",
    "    print('number of codes, size of reconstructed images = ', patches.shape)\n",
    "\n",
    "    plt.matshow(sparse_code[:N_show, :])\n",
    "    plt.show()\n",
    "    fig, axs = show_data(data[:max_patches, :])\n",
    "    plt.show()\n",
    "    fig, axs = show_data(patches[:max_patches, :])\n",
    "    plt.show()\n",
    "    fig, axs = show_data(error[:max_patches, :], cmax=np.max(np.abs(patches[:max_patches, :])))\n",
    "    plt.show()\n",
    "    print('average of data patches = ', data.mean(), '+/-', data.mean(axis=1).std())\n",
    "    print('average of residual patches = ', error.mean(), '+/-', error.mean(axis=1).std())\n",
    "    SD = np.sqrt(np.mean(data**2, axis=1))\n",
    "    #SD = np.linalg.norm(data[indx, :])/record_num_batches\n",
    "\n",
    "    print('median energy of data = ', np.median(SD))\n",
    "    print('average energy of data = ', SD.mean(), '+/-', SD.std())\n",
    "    #print('total energy of data = ', np.sqrt(np.sum(data**2)))\n",
    "    #print('total deviation of data = ', np.sum(np.abs(data)))\n",
    "    \n",
    "    SE = np.sqrt(np.mean(error**2, axis=1))\n",
    "    #SE = np.linalg.norm(error)/record_num_batches\n",
    "\n",
    "    print('average energy of residual = ', SE.mean(), '+/-', SE.std())\n",
    "    print('median energy of residual = ', np.median(SE))\n",
    "    #print('total energy of residual = ', np.sqrt(np.sum(error**2)))\n",
    "    #print('total deviation of residual = ', np.sum(np.abs(error)))\n",
    "    print('average gain of coding = ', (SD/SE).mean(), '+/-', (SD/SE).std())\n",
    "    #print('average gain of coding = ', data[indx, :].std()/error.std())  \n",
    "    \n",
    "    return SD, SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.691Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shl_scripts.shl_encode import sparse_encode\n",
    "if dico_homeo.P_cum is None: \n",
    "    gain = np.ones(shl.n_dictionary)\n",
    "else:\n",
    "    gain = None\n",
    "dico_rec = dico_homeo.dictionary\n",
    "\n",
    "for P_cum_rec, gain_rec in zip([None, P_cum_zeroeffect, dico_homeo.P_cum], [np.ones(shl.n_dictionary), None, gain]):\n",
    "    sparse_code = sparse_encode(data[indx, :], dico_rec, P_cum=P_cum_rec, C=C, \n",
    "                                 l0_sparseness=l0_sparseness, gain=gain_rec)   \n",
    "\n",
    "    SD, SE = print_stats(data[indx, :], dico_homeo.dictionary, sparse_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new coefficients by shuffling and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.694Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def shuffling(data, sparse_code, dico):\n",
    "    if do_random:\n",
    "        from shl_scripts.shl_encode import inv_quantile, inv_rescaling\n",
    "        sparse_code_bar = inv_rescaling(inv_quantile(dico.P_cum, np.random.rand(sparse_code.shape[0], sparse_code.shape[1])), C=C)\n",
    "    else:\n",
    "        sparse_code = sparse_encode(data, dico.dictionary, P_cum=dico.P_cum, C=C, \n",
    "                                     l0_sparseness=l0_sparseness_noise, gain=None)   \n",
    "\n",
    "        sparse_code_bar = sparse_code.copy()\n",
    "        \n",
    "        sparse_code_bar = sparse_code_bar.T\n",
    "        np.random.shuffle(sparse_code_bar)\n",
    "        sparse_code_bar = sparse_code_bar.T\n",
    "        \n",
    "        if do_double_shuffle:\n",
    "            np.random.shuffle(sparse_code_bar)\n",
    "\n",
    "\n",
    "    print('average non-zeros', np.count_nonzero(sparse_code_bar, axis=0)[:N_show])\n",
    "    print('average non-zeros', np.count_nonzero(sparse_code, axis=0)[:N_show])\n",
    "\n",
    "    print(sparse_code_bar.shape)\n",
    "\n",
    "    plt.matshow(sparse_code_bar[:N_show, :])\n",
    "    plt.show()\n",
    "\n",
    "    def threshold(sparse_code, l0_sparseness):\n",
    "        thr = np.percentile(sparse_code, 100 * (1 - l0_sparseness/shl.n_dictionary ), axis=1)\n",
    "        return (sparse_code>thr[:, np.newaxis])\n",
    "\n",
    "    sparse_code_bar_high = threshold(sparse_code_bar, l0_sparseness) * sparse_code_bar\n",
    "    plt.matshow(sparse_code_bar_high[:N_show, :])\n",
    "    plt.show()\n",
    "    return sparse_code_bar, sparse_code_bar_high\n",
    "\n",
    "def pipeline(sparse_code_bar, sparse_code_bar_high, dico, index):\n",
    "\n",
    "    patches_bar = sparse_code_bar @ dico.dictionary\n",
    "    SD = np.sqrt(np.mean(patches_bar**2, axis=1))\n",
    "\n",
    "\n",
    "    P_cum_rec = dico.P_cum\n",
    "    if P_cum_rec is None: \n",
    "        gain_rec = np.ones(shl.n_dictionary)\n",
    "    else:\n",
    "        gain_rec = None\n",
    "\n",
    "    sparse_code_rec = sparse_encode(patches_bar, dico.dictionary, P_cum=P_cum_rec, C=C, \n",
    "                                     l0_sparseness=l0_sparseness, gain=gain_rec)   \n",
    "\n",
    "    print('average non-zeros', np.count_nonzero(sparse_code_bar, axis=0)[:N_show])\n",
    "    print('average non-zeros', np.count_nonzero(sparse_code_bar_high, axis=0)[:N_show])\n",
    "    print('average non-zeros', np.count_nonzero(sparse_code_rec, axis=0)[:N_show])\n",
    "    \n",
    "    \n",
    "    SD, SE = print_stats(patches_bar, dico.dictionary, sparse_code_rec)\n",
    "    \n",
    "    plt.matshow(sparse_code_rec[:N_show, :])\n",
    "    plt.show()\n",
    "\n",
    "    print('mean deviation of coefficients = ', np.mean(np.abs(sparse_code_bar)), np.mean(np.abs(sparse_code_bar_high)), np.mean(np.abs(sparse_code_rec)))\n",
    "    print('total deviation of coefficients = ', np.mean(np.abs(sparse_code_bar_high-sparse_code_rec)))\n",
    "\n",
    "    if not dico.P_cum is None: \n",
    "        from shl_scripts.shl_encode import quantile, rescaling\n",
    "\n",
    "        q_rec = quantile(dico.P_cum, rescaling(sparse_code_rec, C=C), stick, do_fast=False)\n",
    "        q_bar = quantile(dico.P_cum, rescaling(sparse_code_bar_high, C=C), stick, do_fast=False)\n",
    "\n",
    "        print('mean deviation of quantiles = ', np.mean(np.abs(q_bar)))\n",
    "        print('mean deviation of quantiles = ', np.mean(np.abs(q_rec)))\n",
    "        print('total deviation of quantiles = ', np.mean(np.abs(q_bar-q_rec)))\n",
    "        print('ratio deviation of quantiles = ', np.mean(np.abs(q_bar-q_rec))/np.mean(np.abs(q_bar)))\n",
    "        aerror = np.mean(np.abs(q_bar-q_rec))/np.mean(np.abs(q_bar))\n",
    "    else:\n",
    "        aerror = np.nan\n",
    "\n",
    "    perror = 1 - np.mean( (sparse_code_bar>0) == (sparse_code_rec>0))\n",
    "    print('proba incorrect coefficients = ', perror)\n",
    "\n",
    "    perror_high = 1 - np.mean( (sparse_code_bar_high > 0) == (sparse_code_rec>0))\n",
    "    print('proba incorrect coefficients (strong) = ', perror_high)\n",
    "    \n",
    "    return pd.DataFrame({'error':[(SD/SE).mean()],\n",
    "                               'aerror':[aerror],\n",
    "                               'perror':[perror],\n",
    "                               'perror_high':[perror_high]\n",
    "                                        },\n",
    "                                index=[index])\n",
    "\n",
    "sparse_code_bar, sparse_code_bar_high = shuffling(data[indx, :], sparse_code, dico_homeo)\n",
    "record = pipeline(sparse_code_bar, sparse_code_bar_high, dico_homeo, index='homeo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.697Z"
    }
   },
   "outputs": [],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing to the learning without homeostasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.700Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "homeo_method = 'None'\n",
    "matname = tag + '_' + homeo_method\n",
    "\n",
    "opts = dict(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, n_iter=n_iter, \n",
    "            homeo_method=homeo_method, \n",
    "            C=C, nb_quant=nb_quant, P_cum=None, verbose=verbose)\n",
    "\n",
    "from shl_scripts.shl_experiments import SHL\n",
    "shl_nohomeo = SHL(**opts)\n",
    "data = shl_nohomeo.get_data(matname=tag)\n",
    "dico_nohomeo = shl_nohomeo.learn_dico(data=data, list_figures=list_figures, matname=matname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.703Z"
    }
   },
   "outputs": [],
   "source": [
    "if not dico_nohomeo.P_cum is None: \n",
    "    from shl_scripts.shl_tools import plot_P_cum\n",
    "    fig, ax = plot_P_cum(dico_nohomeo.P_cum, ymin=ymin, verbose=False, alpha=.15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.709Z"
    }
   },
   "outputs": [],
   "source": [
    "P_cum_rec = None \n",
    "gain_rec = np.ones(shl_nohomeo.n_dictionary)\n",
    "dico_rec = dico_nohomeo.dictionary\n",
    "\n",
    "sparse_code = sparse_encode(data[indx, :], dico_rec, P_cum=P_cum_rec, C=C, \n",
    "                                 l0_sparseness=l0_sparseness, gain=gain_rec)   \n",
    "\n",
    "print_stats(data[indx, :], dico_nohomeo.dictionary, sparse_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and shuffling the coefficients"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T16:12:17.790250Z",
     "start_time": "2018-02-14T15:52:53.296Z"
    }
   },
   "source": [
    "if do_random:\n",
    "    from shl_scripts.shl_encode import inv_quantile, inv_rescaling\n",
    "    sparse_code_bar = inv_rescaling(inv_quantile(dico_homeo.P_cum, np.random.rand(sparse_code.shape[0], sparse_code.shape[1])), C=C)\n",
    "else:\n",
    "    sparse_code = sparse_encode(data[indx, :], dico_nohomeo.dictionary, P_cum=P_cum_rec, C=C, \n",
    "                                 l0_sparseness=l0_sparseness_noise, gain=gain_rec)   \n",
    "    sparse_code_bar = sparse_code.copy()\n",
    "\n",
    "    np.random.shuffle(sparse_code_bar)\n",
    "\n",
    "    if do_double_shuffle:\n",
    "        sparse_code_bar = sparse_code_bar.T\n",
    "        np.random.shuffle(sparse_code_bar)\n",
    "        sparse_code_bar = sparse_code_bar.T\n",
    "\n",
    "    #sparse_code_bar /= sparse_code_bar.max(axis=1)[:, np.newaxis]\n",
    "    print(sparse_code_bar.shape)\n",
    "plt.matshow(sparse_code_bar[:N_show, :])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T16:12:17.794241Z",
     "start_time": "2018-02-14T15:52:53.300Z"
    }
   },
   "source": [
    "sparse_code_bar_high = threshold(sparse_code_bar, l0_sparseness) * sparse_code_bar\n",
    "plt.matshow(sparse_code_bar_high[:N_show, :])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T16:12:17.796767Z",
     "start_time": "2018-02-14T15:52:53.305Z"
    }
   },
   "source": [
    "patches_bar = sparse_code_bar @ dico_nohomeo.dictionary\n",
    "sparse_code_rec = sparse_encode(patches_bar, dico_nohomeo.dictionary, P_cum=dico_nohomeo.P_cum, C=C, \n",
    "                                 l0_sparseness=l0_sparseness, gain=gain_rec)   \n",
    "\n",
    "print('average non-zeros', np.count_nonzero(sparse_code_bar, axis=0)[:N_show])\n",
    "print('average non-zeros', np.count_nonzero(sparse_code_bar_high, axis=0)[:N_show])\n",
    "print('average non-zeros', np.count_nonzero(sparse_code_rec, axis=0)[:N_show])\n",
    "\n",
    "plt.matshow(sparse_code_rec[:N_show, :])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T16:12:17.799134Z",
     "start_time": "2018-02-14T15:52:53.311Z"
    }
   },
   "source": [
    "print('mean deviation of coefficients = ', np.mean(np.abs(sparse_code_rec)))\n",
    "print('total deviation of coefficients = ', np.mean(np.abs(sparse_code_bar_high-sparse_code_rec)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T16:12:17.801029Z",
     "start_time": "2018-02-14T15:52:53.316Z"
    }
   },
   "source": [
    "if dico_nohomeo.P_cum is None:\n",
    "    from shl_scripts.shl_learn import get_P_cum\n",
    "    dico_nohomeo.P_cum = get_P_cum(sparse_code_bar, C=C, nb_quant=nb_quant)\n",
    "    \n",
    "q_rec = quantile(dico_nohomeo.P_cum, rescaling(sparse_code_rec, C=C), stick, do_fast=False)\n",
    "q_bar = quantile(dico_nohomeo.P_cum, rescaling(sparse_code_bar_high, C=C), stick, do_fast=False)\n",
    "\n",
    "print('mean deviation of quantiles = ', np.mean(np.abs(q_bar)))\n",
    "print('mean deviation of quantiles = ', np.mean(np.abs(q_rec)))\n",
    "print('total deviation of quantiles = ', np.mean(np.abs(q_bar-q_rec)))\n",
    "print('ratio deviation of quantiles = ', np.mean(np.abs(q_bar-q_rec))/np.mean(np.abs(q_bar)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T16:12:17.803162Z",
     "start_time": "2018-02-14T15:52:53.321Z"
    }
   },
   "source": [
    "perror = np.mean( (sparse_code_bar>0) == (sparse_code_rec>0))\n",
    "print('proba correct coefficients = ', perror)\n",
    "#sparse_code_bar_high = threshold(sparse_code_bar, l0_sparseness) * sparse_code_bar\n",
    "#sparse_code_rec_high = threshold(sparse_code_rec, l0_sparseness) * sparse_code_rec\n",
    "#print('proba correct coefficients (high) = ', np.mean( (sparse_code_bar_high>0) == (sparse_code_rec_high>0)))      \n",
    "#perror_high = 1 - np.mean( threshold(sparse_code_bar, l0_sparseness) == (sparse_code_rec>0))\n",
    "perror_high = 1 - np.mean( (sparse_code_bar_high > 0) == (sparse_code_rec>0))\n",
    "print('proba correct coefficients (strong) = ', perror_high)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T21:40:34.819130Z",
     "start_time": "2018-02-14T21:40:34.489012Z"
    }
   },
   "source": [
    "print_stats(patches_bar, dico_nohomeo.dictionary, sparse_code_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.852Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([record, pipeline(sparse_code_bar, sparse_code_bar_high, dico_nohomeo, index='nohomeo')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-06T16:02:13.858Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, shl_scripts, pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
